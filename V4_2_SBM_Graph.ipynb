{"cells":[{"cell_type":"markdown","metadata":{"id":"rTpcq7Jsivnu"},"source":["# Stocatistic block model and Graph neural network"],"id":"rTpcq7Jsivnu"},{"cell_type":"markdown","metadata":{"id":"02KR-nSZtB62"},"source":["V 4.2 adds a new type of GNN"],"id":"02KR-nSZtB62"},{"cell_type":"markdown","metadata":{"id":"6P5tw84dtCWt"},"source":["V 4.1 is the version for the condivision with other people"],"id":"6P5tw84dtCWt"},{"cell_type":"markdown","metadata":{"id":"FZhrEVVPi7nI"},"source":["## Download pack"],"id":"FZhrEVVPi7nI"},{"cell_type":"markdown","metadata":{"id":"b8b8fed2"},"source":["### Pack for colab"],"id":"b8b8fed2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ri9qmLI8uXKe","scrolled":true},"outputs":[],"source":["!pip install torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html"],"id":"Ri9qmLI8uXKe"},{"cell_type":"markdown","metadata":{"id":"HXbSJL2VjJLC"},"source":["## Initialization of the program"],"id":"HXbSJL2VjJLC"},{"cell_type":"markdown","metadata":{"id":"5t_yiO06jdL0"},"source":["### Import libraries"],"id":"5t_yiO06jdL0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6f3895d3"},"outputs":[],"source":["# Importing module\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from sklearn.cluster import KMeans\n","from torch_geometric.nn import GCNConv\n","from tqdm.notebook import trange\n","from itertools import permutations\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"id":"6f3895d3"},{"cell_type":"markdown","metadata":{"id":"wVx2qdTZjkOf"},"source":["### Functions"],"id":"wVx2qdTZjkOf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vEG9TLjGoke"},"outputs":[],"source":["# Show the adiacency matrix\n","def show(A):\n","    f = plt.figure()\n","    f.set_figwidth(7.5)\n","    f.set_figheight(7.5)\n","    plt.imshow(A, cmap='Greys', interpolation='nearest')\n","    plt.axis('off')\n","    plt.show()\n","\n","# Generation of a random graph for the SBM problem with 2 communities\n","def random_graph_generator(n, p, q, k, graph_num, use_MSE=False, shuffle=True):\n","    np.random.seed(graph_num)\n","\n","    # Number of nodes per cluster\n","    n_c = n // k\n","\n","    # Initialize labels and indices\n","    labels = np.reshape(np.indices((k, n_c))[0,:,:], n)\n","    indices = np.arange(n)\n","\n","    # Initialize Adjacency matrix and the triangular indices\n","    A = np.zeros((n, n))\n","    triu_inds = np.triu_indices(n_c, k=1)\n","\n","    # Fill the upper triangle\n","    for i in range(k):\n","        for j in range(i, k):\n","            if j == i:\n","                # in cluster\n","                cluster_i = np.random.choice([0, 1], size=(n_c * (n_c - 1) // 2), p = [1-p, p])\n","                A[triu_inds[0] + i * n_c, triu_inds[1] + i * n_c] = cluster_i\n","            else:\n","                # out of clusters\n","                between_clusters_i_j = np.random.choice([0, 1], size=(n_c, n_c), p=[1-q, q])\n","                A[n_c * i : n_c * (i + 1), n_c * j : n_c * (j + 1)] = between_clusters_i_j\n","\n","    # Fill the under triangle\n","    A = A + A.T\n","\n","    if shuffle:\n","        # Shuffle the adjacency matrix\n","        np.random.shuffle(indices)\n","        A = A[indices][:, indices]\n","\n","        # Shuffle the clusters\n","        labels = labels[indices]\n","\n","    # initialize the label list and the permutations\n","    labels_list = []\n","    perm = list(permutations(range(k)))\n","\n","    if use_MSE:\n","        labels = labels * 2 - 1\n","        labels_list.append(labels)\n","        labels_list.append(-labels)\n","    else:\n","        new_labels = np.zeros(n).astype(int)\n","\n","        for i in range(len(perm)):\n","            # Permute the labels\n","            for l in range(k):\n","                new_labels[labels == l] = perm[i][l]\n","\n","            # initialize the array for the loss\n","            labels_i = np.zeros((n,k))\n","\n","            # create the labels for the cross entropy loss\n","            for j in range(n):\n","                labels_i[j, new_labels[j]] = 1\n","\n","            labels_list.append(labels_i)\n","\n","    # Create the edge index\n","    edge_index = np.array(np.nonzero(A))\n","\n","    return A, edge_index, labels_list\n","\n","# Training\n","def training_detection(model, optim, criterion, n, p, q, k, train_graph_num, par_lambda, print_loss = True, use_MSE=False, shuffle=False):\n","    checkpoint_print = int(train_graph_num/10)\n","    checkpoint_save = int(train_graph_num/100)\n","\n","    X = torch.from_numpy(np.eye(n)).float().to(device)\n","\n","    losses = []\n","    mean_losses = []\n","\n","    for graph_num in trange(train_graph_num):\n","\n","        # Initilize a training graph\n","        A, edge_index, labels_list = random_graph_generator(n, p, q, k, graph_num, use_MSE=use_MSE, shuffle=shuffle)\n","        edge_index = torch.from_numpy(edge_index).long().to(device)\n","        labels_list = [torch.from_numpy(item).float().to(device) for item in labels_list]\n","\n","        # zero grad\n","        optim.zero_grad()\n","\n","        # make the predictions\n","        predictions = model(X, edge_index, use_MSE)[0]\n","\n","        # Calculating the loss\n","        if use_MSE:\n","            predictions = torch.reshape(predictions, (-1,))\n","            loss = min(criterion(predictions, labels_list[0]), criterion(predictions, labels_list[1]))\n","        else:\n","            loss = criterion(predictions, labels_list[0])\n","            for i in range(1, len(labels_list)):\n","                loss = min(loss, criterion(predictions, labels_list[i]))\n","\n","        # backward pass and gradient step\n","        loss.backward()\n","        optim.step()\n","\n","        # Save temporary losses losses and embeddings\n","        if print_loss:\n","            losses.append(loss.to('cpu').detach().numpy())\n","\n","        if print_loss:\n","            if (graph_num + 1) % checkpoint_save == 0 or (graph_num + 1) == train_graph_num:\n","                mean_loss = sum(losses) / len(losses)\n","                mean_losses.append(mean_loss)\n","                losses = []\n","\n","        # Print losses\n","        if print_loss:\n","            if (graph_num + 1) % checkpoint_print == 0:\n","                print(f\"\\tGraph trained: {graph_num+1:,}\\t\\tLoss: {mean_loss:.4f}\")\n","\n","    if print_loss:\n","        f = plt.figure()\n","        f.set_figwidth(10)\n","        f.set_figheight(5)\n","        plt.title('Train loss')\n","        plt.plot(mean_losses, \"b\")\n","        plt.show()\n","\n","    return mean_losses\n","\n","# make predictions\n","def result(n, p, q, k, model, train_graph_num, test_graph_num, rand_lab=False,\n","           istogram=True, use_MSE = False, use_deg=False, shuffle=True, simplier = False):\n","    accuracys = []\n","    X = torch.from_numpy(np.eye(n)).float().to(device)\n","\n","\n","    for graph_num in trange(train_graph_num, train_graph_num + test_graph_num):\n","        A, edge_index, labels_list = random_graph_generator(n, p, q, k, graph_num, use_MSE=use_MSE, shuffle=shuffle)\n","        edge_index_test = torch.from_numpy(edge_index).long().to(device)\n","\n","        if use_deg:\n","            X = torch.from_numpy(np.diag(np.sum(A, axis=0)) / (np.mean(np.sum(A, axis=0)))).float().to(device)\n","\n","        # Random label or model\n","        if rand_lab:\n","            predictions = np.zeros((n, k))\n","            interval = n // k\n","            for i in range(0, k):\n","                predictions[(0 + interval * i):(interval + interval * i), i] = 1\n","        else:\n","            predictions = model(X, edge_index_test, use_MSE)[0].cpu().detach().numpy()\n","\n","        # initialize results and corrects££\n","        results = np.zeros(n).astype(int)\n","        corrects = np.zeros(n)\n","\n","        # Compare predictions and labels and calculate accuracy\n","        if use_MSE:\n","            # correctness of the model\n","            predictions = np.reshape(predictions, -1)\n","            accuracy = max(np.mean(np.array(np.sign(predictions) == np.sign(labels_list[0])).astype(int)), np.mean(np.array(np.sign(predictions) == np.sign(labels_list[1])).astype(int)))\n","            accuracys.append(accuracy * 100)\n","\n","        else:\n","            # correctness of the model\n","            results = [np.argmax(predictions[i,:]) for i in range(n)]\n","            corrects = [int(np.argmax(labels_list[0][i,:]) == results[i]) for i in range(n)]\n","\n","            for j in range(1, len(labels_list)):\n","                corrects_2 = [int(np.argmax(labels_list[j][i,:]) == results[i]) for i in range(n)]\n","                if np.mean(corrects) * 100 < np.mean(corrects_2) * 100:\n","                    corrects = corrects_2\n","\n","            # Accuracy for the Cross Entropy metod\n","            accuracy = np.mean(corrects) * 100\n","            accuracys.append(accuracy)\n","\n","    mean_accuracy = sum(accuracys) / len(accuracys)\n","\n","    print(f\"\\nMean accuracy: {mean_accuracy:.2f} %\\n\\n\")\n","\n","    # Istogram of accuracys\n","    if istogram:\n","        f = plt.figure()\n","        f.set_figwidth(20)\n","        f.set_figheight(8)\n","        plt.xlabel('SNR')\n","        plt.ylabel('Accuracy')\n","        plt.title('Performance of the model')\n","        plt.hist(accuracys, bins=20)\n","        plt.show()\n","\n","    return accuracys, mean_accuracy\n","\n","# Spectral cluster\n","def spectral_clustering(n, q, p, k, train_graph_num, test_graph_num, shuffle=False):\n","    accuracys = []\n","\n","    for graph_num in trange(train_graph_num, train_graph_num + test_graph_num):\n","        A, edge_index, labels_list = random_graph_generator(n, p, q, k, graph_num, shuffle=shuffle)\n","        D = np.diag(A.sum(axis=0))\n","\n","        # graph laplacian\n","        L = D - A\n","\n","        # eigenvalues and eigenvectors\n","        vals, vecs = np.linalg.eig(L)\n","\n","        # sort these based on the eigenvalues\n","        vecs = vecs[:,np.argsort(vals)]\n","        vals = vals[np.argsort(vals)]\n","\n","        # Apply kmeans algorithm\n","        kmeans = KMeans(n_clusters=k, n_init=15)\n","        kmeans.fit(vecs[:,1:k])\n","        results = kmeans.labels_\n","\n","        # Count the correct label\n","        corrects = [int(np.argmax(labels_list[0][i,:]) == results[i]) for i in range(n)]\n","        for j in range(1, len(labels_list)):\n","            corrects_2 = [int(np.argmax(labels_list[j][i,:]) == results[i]) for i in range(n)]\n","            if np.mean(corrects) * 100 < np.mean(corrects_2) * 100:\n","                corrects = corrects_2\n","\n","        accuracy = np.mean(corrects) * 100\n","        accuracys.append(accuracy)\n","\n","    std_spectral_clustering = np.std(accuracys)\n","    mean_accuracy = sum(accuracys) / len(accuracys)\n","\n","    print(f\"\\nMean accuracy: {mean_accuracy:.2f} %\\n\\n\")\n","\n","    # Istogram of accuracys\n","    f = plt.figure()\n","    f.set_figwidth(15)\n","    f.set_figheight(5)\n","    plt.title('Accuracy')\n","    plt.hist(accuracys, bins=20)\n","    plt.show()\n","\n","    return mean_accuracy, accuracys, std_spectral_clustering"],"id":"1vEG9TLjGoke"},{"cell_type":"markdown","metadata":{"id":"RR4M4vlqXla8"},"source":["## Generate a SBM problem"],"id":"RR4M4vlqXla8"},{"cell_type":"markdown","metadata":{"id":"IgdXWI0tjsNJ"},"source":["### Initialization of the parameters"],"id":"IgdXWI0tjsNJ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fC34mVRiNPwl"},"outputs":[],"source":["# Initialization\n","# number of vertices$\n","n = 300\n","# probability in clusters\n","p = 10 / n\n","# Probability between clusters\n","q = 5 / n\n","# Number of cluster\n","k = 2\n","# Number of hidden features\n","h = 1\n","# Lambda parameter for the regularization (if 0 train without regularization)\n","par_lambda = 0\n","# Initialize seed\n","seed = 2023\n","# Number of graph to train (thousands)\n","train_graph_num = 10\n","# Number of graph to test (thousands)\n","test_graph_num = 1\n","# If true use MSE as loss function, if false use cross entropy loss\n","use_MSE = True\n","# if 1 use the normal model if 2 use the linear model\n","model_type = 2\n","\n","\n","\n","# AUTO INITIALIZATION\n","# True number of graph\n","train_graph_num = int(train_graph_num * 1000)\n","test_graph_num = int(test_graph_num * 1000)\n","\n","# Total number of graph\n","tot_graph_num = train_graph_num + test_graph_num\n","\n","# Use MSE only with 2 clusters\n","if k != 2:\n","    use_MSE = False\n","\n","# Parameters a and b\n","a = n * p\n","b = n * q\n","\n","# check if n is divisible by k, if not change the value of n\n","if not n % k == 0:\n","    n = (n // k) * k\n","\n","# SNR\n","snr = ((a - b) ** 2)/(k * (a + (k - 1) * b))\n","\n","# is true shuffle the row and columns of the adjacency matrix (DO NOT CHANGE IT: Experiments without shuffle aren't relevant)\n","shuffle = True\n","\n","print(f\"Parameter a: {a:.2f}\\tParameter b: {b:.2f}\\tSNR: {snr:.2f}\\tProbability p: {p:.2f}\\tProbability q: {q:.2f}\\tn: {n}\")"],"id":"fC34mVRiNPwl"},{"cell_type":"markdown","metadata":{"id":"95_VOnbVFFst"},"source":["### GNN Training"],"id":"95_VOnbVFFst"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldEhkP5oi0y3"},"outputs":[],"source":["class GNN(nn.Module):\n","    def __init__(self, h, k):\n","        super(GNN, self).__init__()\n","        torch.manual_seed(seed)\n","\n","        self.conv_initial = GCNConv(n, h)\n","        #self.conv_1 = GCNConv(h, h)\n","        if use_MSE:\n","            self.conv_final = GCNConv(h, 1, bias=False)\n","        else:\n","            self.conv_final = GCNConv(h, k, bias=False)\n","\n","    def forward(self, X, edge_index, use_MSE):\n","        preactivations = self.conv_initial(X, edge_index)\n","        preactivations = F.relu(preactivations)\n","        X = preactivations\n","        #X = self.conv_1(X, edge_index)\n","        #X = F.relu(X)\n","        X = self.conv_final(X, edge_index)\n","        if not use_MSE:\n","            X = F.softmax(X, dim=1)\n","\n","        return X, preactivations"],"id":"ldEhkP5oi0y3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxqfJIv_tUwc"},"outputs":[],"source":["class GNN_2(nn.Module):\n","    def __init__(self, h, k):\n","        super(GNN_2, self).__init__()\n","        torch.manual_seed(seed)\n","\n","        self.conv_initial = GCNConv(n, h, bias=False)\n","        if use_MSE:\n","            self.conv_final = nn.Linear(h, 1, bias=False)\n","        else:\n","            self.conv_final = GCNConv(h, k, bias=False)\n","\n","    def forward(self, X, edge_index, use_MSE):\n","        preactivations = self.conv_initial(X, edge_index)\n","        X = preactivations\n","        # X = F.relu(X)\n","        # X = self.conv_1(X, edge_index)\n","        # X = F.relu(X)\n","        X = self.conv_final(X)\n","        if not use_MSE:\n","            X = F.softmax(X, dim=1)\n","\n","        return X, preactivations"],"id":"KxqfJIv_tUwc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"93m0OSP4uK50"},"outputs":[],"source":["# initialize the model\n","if model_type == 1:\n","    model = GNN(h, k).to(device)\n","else:\n","    model = GNN_2(h, k).to(device)"],"id":"93m0OSP4uK50"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8naXeKTWouT-"},"outputs":[],"source":["# initialize the loss and the optimizer\n","if use_MSE:\n","    criterion = torch.nn.MSELoss().to(device)\n","else:\n","    criterion = torch.nn.CrossEntropyLoss().to(device)\n","\n","optim = torch.optim.Adam(model.parameters(), lr = 5e-2, weight_decay = par_lambda)"],"id":"8naXeKTWouT-"},{"cell_type":"code","execution_count":null,"metadata":{"id":"wp3mqoRKL8-T"},"outputs":[],"source":["# Layer to freeze ('none', 'initial' or 'final')\n","name_to_freeze = 'final'\n","\n","for name, param in model.named_parameters():\n","    if name_to_freeze in name:\n","        param.requires_grad = False\n","        print(f'parameter: {name} freezed')\n","    else:\n","        param.requires_grad = True\n","        print(f'parameter: {name} not freezed')"],"id":"wp3mqoRKL8-T"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkYgheOewJTu"},"outputs":[],"source":["# Train the model\n","train_losses = training_detection(model, optim, criterion, n, p, q, k, train_graph_num, par_lambda, print_loss = True, use_MSE = use_MSE, shuffle=shuffle)"],"id":"dkYgheOewJTu"},{"cell_type":"markdown","metadata":{"id":"7ryyOBanP2X-"},"source":["### Results of the training"],"id":"7ryyOBanP2X-"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3KwmeJDGamsd"},"outputs":[],"source":["# Accuracy and other results\n","accuracys, mean_accuracy = result(n, p, q, k, model, train_graph_num, test_graph_num, rand_lab=False, istogram=True, use_MSE = use_MSE, shuffle=shuffle)\n","\n","print(f'n = {n}, p = {p:.2f}, q = {q:.2f}, SNR = {snr:.2f}\\n\\n')\n","print(mean_accuracy)\n","print(np.std(accuracys))"],"id":"3KwmeJDGamsd"},{"cell_type":"markdown","metadata":{"id":"nAY6Y52du-wL"},"source":["### Spectral clustering"],"id":"nAY6Y52du-wL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpEou5Pm15EV"},"outputs":[],"source":["# Accuracy Spectral clustering\n","mean_accuracy_spectral_clustering, accuracys_spectral_clustering, std_spectral_clustering = spectral_clustering(n, q, p, k, train_graph_num, test_graph_num, shuffle=shuffle)"],"id":"FpEou5Pm15EV"},{"cell_type":"markdown","metadata":{"id":"J5WnYyXxvFML"},"source":["### Random assignment\n"],"id":"J5WnYyXxvFML"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAoNGft9aga4"},"outputs":[],"source":["# Accuracy random assignement\n","accuracys, mean_accuracy = result(n, p, q, k, model, train_graph_num, test_graph_num, rand_lab=True, shuffle=shuffle)"],"id":"dAoNGft9aga4"},{"cell_type":"markdown","metadata":{"id":"7u19wFLY8wZr"},"source":["### Training for different values"],"id":"7u19wFLY8wZr"},{"cell_type":"code","execution_count":null,"metadata":{"id":"_KaYVN5yXiX4"},"outputs":[],"source":["accuracy_random = []\n","accuracy_spectral = []\n","accuracy_not_trained = []\n","accuracy_model = []\n","SNR = []\n","\n","random = False\n","spectral = True\n","not_trained = False\n","train_model = False\n","\n","for a in range(1, 14):\n","    # Initialization\n","    # number of vertices$\n","    n = 500\n","    # probability in clusters\n","    p = a / n\n","    # Probability between clusters\n","    q = 1 / n\n","    # Number of cluster\n","    k = 2\n","    # Number of hidden features\n","    h = 50\n","    # Lambda parameter for the regularization (if 0 train without regularization)\n","    par_lambda = 0\n","    # Initialize seed\n","    seed = 2023\n","    # Number of graph to train (thousands)\n","    train_graph_num = 20\n","    # Number of graph to test (thousands)\n","    test_graph_num = 1\n","    # If true use MSE as loss function, if false use cross entropy loss\n","    use_MSE = True\n","\n","    # AUTO INITIALIZATION\n","    # True number of graph\n","    train_graph_num = int(train_graph_num * 1000)\n","    test_graph_num = int(test_graph_num * 1000)\n","\n","    # Total number of graph\n","    tot_graph_num = train_graph_num + test_graph_num\n","\n","    # Use MSE only with 2 clusters\n","    if k != 2:\n","        use_MSE = False\n","\n","    # Parameters a and b\n","    a = n * p\n","    b = n * q\n","\n","    # check if n is divisible by k, if not change the value of n\n","    if not n % k == 0:\n","        n = (n // k) * k\n","\n","    # SNR\n","    snr = ((a - b) ** 2)/(k * (a + (k - 1) * b))\n","    SNR.append(snr)\n","\n","    # is true shuffle the row and columns of the adjacency matrix (DO NOT CHANGE IT: Experiments without shuffle aren't relevant)\n","    shuffle = True\n","\n","    print(f\"Parameter a: {a:.2f}\\tParameter b: {b:.2f}\\tSNR: {snr:.2f}\\tProbability p: {p:.2f}\\tProbability q: {q:.2f}\\tn: {n}\")\n","\n","    class GNN(nn.Module):\n","        def __init__(self, h, k):\n","            super(GNN, self).__init__()\n","            torch.manual_seed(seed)\n","\n","            self.conv_initial = GCNConv(n, h)\n","            #self.conv_1 = GCNConv(h, h)\n","            if use_MSE:\n","                self.conv_final = GCNConv(h, 1, bias=False)\n","            else:\n","                self.conv_final = GCNConv(h, k, bias=False)\n","\n","        def forward(self, X, edge_index, use_MSE):\n","            preactivations = self.conv_initial(X, edge_index)\n","            preactivations = F.relu(preactivations)\n","            X = preactivations\n","            #X = self.conv_1(X, edge_index)\n","            #X = F.relu(X)\n","            X = self.conv_final(X, edge_index)\n","            if not use_MSE:\n","                X = F.softmax(X, dim=1)\n","\n","            return X, preactivations\n","\n","    # initialize the model\n","    model = GNN(h, k).to(device)\n","\n","    # initialize the loss and the optimizer\n","    if use_MSE:\n","        criterion = torch.nn.MSELoss().to(device)\n","    else:\n","        criterion = torch.nn.CrossEntropyLoss().to(device)\n","\n","    optim = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = par_lambda)\n","\n","    # Layer to freeze ('none', 'initial' or 'final')\n","    name_to_freeze = 'none'\n","\n","    for name, param in model.named_parameters():\n","        if name_to_freeze in name:\n","            param.requires_grad = False\n","            print(f'parameter: {name} freezed')\n","        else:\n","            param.requires_grad = True\n","            print(f'parameter: {name} not freezed')\n","\n","    # Accuracy not trained\n","    if not_trained:\n","        accuracys, mean_accuracy_not_trained = result(n, p, q, k, model, train_graph_num, test_graph_num, rand_lab=False, istogram=False, use_MSE = use_MSE, shuffle=shuffle)\n","        print(f\"Accuracy not trained: {mean_accuracy_not_trained}\")\n","        accuracy_not_trained.append(mean_accuracy_not_trained)\n","\n","    # Train the model\n","    if train_model:\n","        train_losses = training_detection(model, optim, criterion, n, p, q, k, train_graph_num, par_lambda, print_loss = False, use_MSE = use_MSE, shuffle=shuffle)\n","\n","        # Accuracy model\n","        accuracys, mean_accuracy_model = result(n, p, q, k, model, train_graph_num, test_graph_num, rand_lab=False, istogram=False, use_MSE = use_MSE, shuffle=shuffle)\n","        print(f\"Accuracy model: {mean_accuracy_model}\")\n","        accuracy_model.append(mean_accuracy_model)\n","\n","    # Accuracy Spectral clustering\n","    if spectral:\n","        mean_accuracy_spectral_clustering, accuracys_spectral_clustering, std_spectral_clustering = spectral_clustering(n, q, p, k, train_graph_num, test_graph_num, shuffle=shuffle)\n","        print(f\"Accuracy spectral clustering: {mean_accuracy_spectral_clustering}\")\n","        accuracy_spectral.append(mean_accuracy_spectral_clustering)\n","\n","    # Accuracy random assignement\n","    if random:\n","        accuracys, mean_accuracy_random = result(n, p, q, k, model, train_graph_num, test_graph_num, rand_lab=True, istogram=False, shuffle=shuffle)\n","        print(f\"Accuracy random: {mean_accuracy_random}\")\n","        accuracy_random.append(mean_accuracy_random)"],"id":"_KaYVN5yXiX4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XJX7eFRumbY"},"outputs":[],"source":["# Plot the accuracy for n = 500\n","accuracy_simple_ones = [51.85339999999996, 52.051999999999936, 52.249600000000044, 52.41960000000003, 52.650999999999996, 52.84460000000002, 53.01439999999998, 53.126200000000075, 53.26839999999993, 53.4268, 53.56980000000002, 53.784600000000054, 53.87540000000007]\n","SNR_ones = [0.0, 0.023809523809523808, 0.09090909090909091, 0.1956521739130435, 0.3333333333333333, 0.5, 0.6923076923076923, 0.9074074074074074, 1.1428571428571428, 1.396551724137931, 1.6666666666666667, 1.9516129032258065, 2.25, 2.5606060606060606, 2.8823529411764706, 3.2142857142857144, 3.5555555555555554, 3.9054054054054053, 4.2631578947368425, 4.628205128205129]\n","accuracy_model_MSE = [51.767399999999974, 52.06679999999999, 52.42039999999999, 52.78120000000004, 53.05960000000003, 53.56259999999999, 53.81000000000004, 54.404599999999974, 54.58599999999997, 55.1725999999999, 55.93039999999997, 56.10800000000006, 56.48200000000003]\n","accuracy_spectral = [50.20559999999943, 50.20759999999948, 50.20779999999949, 50.207199999999446, 50.20759999999943, 50.210599999999445, 50.21399999999945, 50.21559999999942, 50.21239999999943, 50.25559999999948, 50.49459999999945, 50.68979999999948, 51.579599999999424, 53.09699999999944, 56.241199999999324, 60.715399999999256]\n","accuracy_random = [51.788800000000165, 51.788800000000165, 51.788800000000165, 51.788800000000165, 51.788800000000165, 51.788800000000165, 51.788800000000165, 51.788800000000165, 51.788800000000165, 51.788800000000165, 51.788800000000165, 51.788800000000165, 51.788800000000165]\n","accuracy_model_CE = [51.74299999999998, 52.19360000000001, 52.541399999999996, 52.844399999999965, 53.19799999999996, 53.41420000000003, 54.024000000000036, 54.652800000000056, 54.70040000000002, 55.465200000000024, 55.74540000000005, 56.50160000000001, 56.92839999999994]\n","SNR = [5.142857142857143, 4.033333333333333, 3.125, 2.3823529411764706, 1.7777777777777777, 1.2894736842105263, 0.9, 0.5952380952380952, 0.36363636363636365, 0.1956521739130435, 0.08333333333333333, 0.02, 0.0]\n","accuracy_2_layer_ones = [51.763199999999955, 52.09879999999997, 52.44339999999999, 52.744999999999855, 53.15260000000003, 53.501600000000046, 53.881199999999986, 54.2576, 54.70599999999996, 55.081799999999966, 55.58519999999994, 55.9832, 56.42740000000004]\n","accuracy_itermediate = [51.902199999999944, 51.98879999999992, 52.22960000000001, 52.38780000000003, 52.6094, 52.76920000000004, 52.93219999999997, 53.067399999999985, 53.251199999999976, 53.406999999999954, 53.5058, 53.676, 53.77439999999994]\n","SNR_2 = [0.0, 0.16666666666666666, 0.5, 0.9, 1.3333333333333333, 1.7857142857142858, 2.25, 2.7222222222222223, 3.2, 3.6818181818181817, 4.166666666666667, 4.653846153846154, 5.142857142857143]\n","SNR_spectral = [0.045454545454545456, 0.16666666666666666, 0.34615384615384615, 0.5714285714285714, 0.8333333333333334, 1.125, 1.4411764705882353, 1.7777777777777777, 2.1315789473684212, 2.5, 2.880952380952381, 3.272727272727273, 3.6739130434782608, 4.083333333333333, 4.5, 4.923076923076923]\n","\n","f_1 = plt.figure()\n","f_1.set_figwidth(20)\n","f_1.set_figheight(10)\n","plt.xlabel('SNR')\n","plt.ylabel('Mean accuracy')\n","plt.title('Accuracy with different SNR for 2 communities (20k graph trained)')\n","plt.plot(SNR_2, accuracy_model_MSE, marker='o', label=\"Model trained with MSE loss\", linewidth=3)\n","plt.plot(SNR_2, accuracy_model_CE, marker='o', label=\"Model trained with CE loss\", linewidth=3)\n","plt.plot(SNR_spectral, accuracy_spectral, marker='o', label=\"Spectral Clustering\", linewidth=3)\n","plt.plot(SNR, accuracy_random, marker='o', label=\"Random assignement\", linewidth=3)\n","plt.plot(SNR_2, accuracy_simple_ones, marker='o', label=\"Model Mw\", linewidth=3)\n","plt.plot(SNR_2, accuracy_itermediate, marker='o', label=\"Model MWθ\", linewidth=3)\n","plt.plot(SNR_2, accuracy_2_layer_ones, marker='o', label=\"2 layer model f(G)\", linewidth=3)\n","plt.legend(loc=\"upper left\")\n","f_1.savefig('Accuracy trivial model and intermediate 500 for different SNR.pdf')\n","plt.show()"],"id":"6XJX7eFRumbY"}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1-tjeZt0n_VIlBIk_ql4_jTJt-xOc7oNT","timestamp":1686138758401}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"28db2cdea796381823031ea64014c20b5b1d93b04c2797bd17c518394bba0663"}}},"nbformat":4,"nbformat_minor":5}